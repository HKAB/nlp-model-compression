
# NLP Compression

Experiments for language model compression


## Features

- Quantization
- Movement pruning for RoBERTa (source code from [huggingface](https://github.com/huggingface/transformers/tree/master/examples/research_projects/movement-pruning))
- LayerDrop
  
## Results

Pretrained phoBERT on NER task: [Google drive](https://drive.google.com/file/d/1-6unZYakOL6z6rCe3N31uLmEM5roQ5Vp/view?usp=sharing)

Pretrained phoBERT on POS task: [Google drive](https://drive.google.com/file/d/1RTEPnz9gtrNxjfaaIxQDnozxFA_5pi8Z/view?usp=sharing)

Report: [Google drive PDF](https://drive.google.com/file/d/1rdP5A6FgLSIgf_1d3703pBtdPzIPRd0M/view?usp=sharing)
