{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b9244a-62aa-44ae-afe8-c663d0dd592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import BertForSequenceClassification\n",
    "from models import BertConfig\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import transformers\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50f3fa8-4523-4c4e-a886-361db3dedd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8791349b-483e-4d21-8158-2ec26237763d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.W_layers_attention.bias', 'classifier.bias', 'bert.W_layers_attention.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a576e0e-d580-4fd6-a030-fb24fc5e87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", \n",
    "                   max_length = 512, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f35119dd-b3dd-4a46-9cec-60e064e9dbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0198, -3.6770, -0.8395,  2.6190, -4.1572,  4.7783,  9.0396, -4.7107,\n",
      "         -3.4140, -1.7998,  0.1924, -6.3741]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2db45fa0-b22f-4785-ab4a-0327a6a68db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 5.4628e-03,  4.2350e-02,  2.4345e-03,  ..., -1.7192e-02,\n",
       "         -2.4809e-02, -3.3402e-02],\n",
       "        [ 7.9426e-03,  1.8272e-02,  3.0775e-03,  ...,  2.6726e-02,\n",
       "         -3.3895e-02, -1.7180e-02],\n",
       "        [ 1.3042e-02, -2.2363e-02, -3.5179e-05,  ..., -2.8074e-02,\n",
       "          3.0295e-02,  5.4064e-03],\n",
       "        ...,\n",
       "        [-4.6543e-02, -7.3823e-03, -1.8623e-03,  ..., -4.0880e-02,\n",
       "         -2.9533e-02,  1.5385e-03],\n",
       "        [-8.2653e-03, -1.5525e-02, -1.6346e-02,  ..., -2.0100e-02,\n",
       "          2.1804e-02,  4.9323e-03],\n",
       "        [-6.1210e-03, -1.7864e-03, -7.5899e-03,  ...,  9.7482e-04,\n",
       "         -1.1968e-02,  2.2976e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.W_layers_attention.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
