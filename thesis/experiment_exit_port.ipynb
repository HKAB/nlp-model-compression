{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca40d2b-9a54-4e04-9f6a-84bd6279bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "# My custom model\n",
    "from models import BertForSequenceClassification\n",
    "from models import BertConfig\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5f4ceb-a543-432e-aa39-38652e90d67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifiers.3.weight', 'exit_port.4.weight', 'exit_port.10.bias', 'exit_port.3.weight', 'classifiers.5.bias', 'classifiers.10.bias', 'classifiers.7.weight', 'classifiers.11.weight', 'classifiers.2.bias', 'exit_port.3.bias', 'exit_port.6.weight', 'exit_port.1.bias', 'classifiers.11.bias', 'classifiers.8.weight', 'exit_port.6.bias', 'exit_port.7.weight', 'exit_port.0.bias', 'exit_port.9.weight', 'classifiers.4.bias', 'exit_port.5.weight', 'classifiers.3.bias', 'classifiers.9.weight', 'classifiers.1.bias', 'exit_port.2.weight', 'classifiers.1.weight', 'classifiers.2.weight', 'exit_port.0.weight', 'classifiers.6.weight', 'classifiers.7.bias', 'classifiers.10.weight', 'exit_port.1.weight', 'classifiers.5.weight', 'exit_port.8.weight', 'classifiers.9.bias', 'exit_port.9.bias', 'exit_port.8.bias', 'exit_port.11.weight', 'classifiers.8.bias', 'classifiers.0.weight', 'exit_port.10.weight', 'exit_port.2.bias', 'exit_port.11.bias', 'exit_port.7.bias', 'classifiers.4.weight', 'classifiers.0.bias', 'exit_port.5.bias', 'classifiers.6.bias', 'exit_port.4.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# change model pretrained path here\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ce5303-9cc9-46dd-8b2e-07142aab3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Hello, my dog is cute and I am the biggest person in the world\", \"Yo yo\"], \n",
    "                   max_length = 128, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor([1, 0]).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a56c5c87-87e1-4272-9d26-4e9e12564a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first phrase\n",
    "outputs = model(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb341385-6170-492d-a437-66c3e025e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Hello, my dog is cute and I am the biggest person in the world\", \"Yo yo wtf\", \"man i got her\"], \n",
    "                   max_length = 128, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor([1, 0, 0]).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff2196eb-252b-479d-a627-a2e0b8a00dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second phrase\n",
    "outputs = model.exit_forward(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea0e35a5-b7ec-45f4-8c30-c294147af651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 2, 1, 2, 3, 2, 1, 1, 2, 2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_exit_decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae60a1d-0451-45bd-82e9-b3cc3e732daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed676c69-743b-4c07-887d-b2a59f946ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.exit_inference(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bca89e24-f9ed-4017-a138-ff680d36d29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stop_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb40b97-5105-4e86-bb13-11f365ed64b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5716, -0.3170]], grad_fn=<AddmmBackward>), hidden_states=(tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
       "           3.8253e-02,  1.6400e-01],\n",
       "         [ 3.7386e-01, -1.5575e-02, -2.4561e-01,  ..., -3.1657e-02,\n",
       "           5.5144e-01, -5.2406e-01],\n",
       "         [ 4.6709e-04,  1.6225e-01, -6.4443e-02,  ...,  4.9443e-01,\n",
       "           6.9413e-01,  3.6286e-01],\n",
       "         ...,\n",
       "         [ 2.6543e-02, -1.7981e-01,  4.8942e-01,  ..., -5.3213e-01,\n",
       "          -2.4651e-01,  4.6587e-01],\n",
       "         [-1.1945e-01, -2.3242e-01,  2.4277e-01,  ..., -4.7397e-01,\n",
       "          -1.3933e-01,  3.1804e-01],\n",
       "         [ 1.1076e-01, -7.5039e-02,  3.2258e-01,  ..., -7.3081e-02,\n",
       "          -5.1369e-01,  2.2897e-01]]], grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.0756,  0.0418, -0.2009,  ...,  0.1857, -0.0269,  0.0443],\n",
       "         [ 0.4123,  0.1266,  0.3189,  ...,  0.4447,  0.9412, -0.3549],\n",
       "         [-0.1503,  0.3128, -0.0155,  ..., -0.0342,  0.7499,  0.3273],\n",
       "         ...,\n",
       "         [-0.0991, -0.2563,  0.6976,  ...,  0.3403, -0.1549,  0.0583],\n",
       "         [-0.1885, -0.2625,  0.5826,  ...,  0.3815, -0.0672, -0.0038],\n",
       "         [-0.0658, -0.2083,  0.6080,  ...,  0.7090, -0.3491, -0.0987]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)), attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b982fe-fa53-4047-a2c5-83700b8251a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.9250, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8aa4153b-cf6d-42ae-8873-0c636540f28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5019,  0.4817]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[-0.5019, 0.4817]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16805582-df8f-4d8f-921e-38b03e5b8f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9009/2430103232.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2722, 0.7278]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d72fb126-12d7-4f83-b76a-3ac73fae9c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc\n"
     ]
    }
   ],
   "source": [
    "if a:\n",
    "    print('cc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
