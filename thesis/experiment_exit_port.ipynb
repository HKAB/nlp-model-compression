{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca40d2b-9a54-4e04-9f6a-84bd6279bd95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "# My custom model\n",
    "from models import BertForSequenceClassification\n",
    "from models import DeeBertForSequenceClassification\n",
    "from models import BertConfig\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639bbcb6-afa9-47cd-b521-12a65318cad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test my BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a5f4ceb-a543-432e-aa39-38652e90d67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['exit_port.5.weight', 'exit_port.9.weight', 'classifiers.6.bias', 'exit_port.2.weight', 'classifiers.7.weight', 'classifiers.11.bias', 'classifiers.3.weight', 'exit_port.8.bias', 'exit_port.0.bias', 'classifiers.1.bias', 'exit_port.2.bias', 'exit_port.1.bias', 'classifiers.5.bias', 'classifiers.2.bias', 'classifiers.9.bias', 'exit_port.1.weight', 'exit_port.11.bias', 'classifiers.8.weight', 'classifiers.0.bias', 'classifiers.3.bias', 'classifiers.1.weight', 'classifiers.10.weight', 'classifiers.4.weight', 'exit_port.4.weight', 'exit_port.0.weight', 'classifiers.7.bias', 'exit_port.3.bias', 'exit_port.8.weight', 'classifiers.8.bias', 'exit_port.3.weight', 'classifiers.4.bias', 'classifiers.6.weight', 'classifiers.2.weight', 'classifiers.0.weight', 'classifiers.10.bias', 'exit_port.7.weight', 'classifiers.5.weight', 'exit_port.5.bias', 'exit_port.7.bias', 'exit_port.11.weight', 'exit_port.6.bias', 'exit_port.10.weight', 'exit_port.6.weight', 'exit_port.4.bias', 'exit_port.9.bias', 'classifiers.11.weight', 'classifiers.9.weight', 'exit_port.10.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# change model pretrained path here\n",
    "config = BertConfig(exit_port_threshold=0.1, entropy_threshold=0.2)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0ce5303-9cc9-46dd-8b2e-07142aab3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Hello, my dog is cute and I am the biggest person in the world\", \"Yo yo\"], \n",
    "                   max_length = 128, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor([1, 0]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a56c5c87-87e1-4272-9d26-4e9e12564a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first phrase\n",
    "outputs = model(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb341385-6170-492d-a437-66c3e025e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Hello, my dog is cute and I am the biggest person in the world\", \"Yo yo wtf\", \"man i got her\"], \n",
    "                   max_length = 128, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor([1, 0, 0]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2196eb-252b-479d-a627-a2e0b8a00dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second phrase\n",
    "outputs = model.exit_forward(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea0e35a5-b7ec-45f4-8c30-c294147af651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stop_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae60a1d-0451-45bd-82e9-b3cc3e732daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed676c69-743b-4c07-887d-b2a59f946ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkab/Desktop/nlp-model-compression/thesis/models/modeling_bert.py:694: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = torch.nn.functional.softmax(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertConfig' object has no attribute 'entropy_threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16363/3824776031.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit_inference_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/nlp-model-compression/thesis/models/modeling_bert.py\u001b[0m in \u001b[0;36mexit_inference_forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1884\u001b[0;31m         outputs, pooled_sequence_outputs = self.bert.exit_inference_forward(\n\u001b[0m\u001b[1;32m   1885\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nlp-model-compression/thesis/models/modeling_bert.py\u001b[0m in \u001b[0;36mexit_inference_forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, exit_port, classifiers)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         )\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         encoder_outputs = self.encoder.exit_inference_forward(\n\u001b[0m\u001b[1;32m   1216\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nlp-model-compression/thesis/models/modeling_bert.py\u001b[0m in \u001b[0;36mexit_inference_forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, exit_port, classifiers, pooler)\u001b[0m\n\u001b[1;32m    701\u001b[0m                 dim=-1)\n\u001b[1;32m    702\u001b[0m             \u001b[0;31m# only support for batch 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mexit_decision\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit_port_threshold\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mentropy\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertConfig' object has no attribute 'entropy_threshold'"
     ]
    }
   ],
   "source": [
    "outputs = model.exit_inference_forward(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca89e24-f9ed-4017-a138-ff680d36d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.stop_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947ab01-c354-4424-b21b-f0db5deb3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb56250-2827-4aa4-b205-a3dca3cdbc5f",
   "metadata": {},
   "source": [
    "# Test DeeBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5e2de-256f-48f1-b975-7e49c8d6b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# change model pretrained path here\n",
    "config = BertConfig(entropy_threshold=0.5)\n",
    "model = DeeBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9333b48-cf37-450d-9219-effdd11ae875",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Hello, my dog is cute and I am the biggest person in the world haha, Excuse me\"], \n",
    "                   max_length = 128, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor([0]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedee9bf-73db-4db3-940d-17d25dd6f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first phrase\n",
    "outputs = model(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe2e60-8452-4ec9-808d-6d54b5333199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second phrase\n",
    "outputs = model.exit_inference_forward(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeed24f-a8fd-4bb0-9a58-30733cb07c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.stop_layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
