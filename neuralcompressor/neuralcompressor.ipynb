{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b2d0cd-d297-40c9-8265-9c48f2fde512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import AdamW\n",
    "import os\n",
    "import transformers\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1066f6a1-a188-473e-bda5-3e33c4256810",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "130b90ff-f46f-450a-82f7-11fceca3cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8779e1c0-96f2-48a2-8cba-232efb5f3d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64001, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phobert.embeddings.word_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "327b08b8-e616-430e-bca4-820c0df14a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134998272"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in phobert.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa42b41-4726-40a0-ba2e-a90565b36d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer hold 36.41%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding layer hold {64001*768/134998272.0*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f99f764-55ad-4b97-8947-3ab82225ca8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89acb832-8759-457a-9793-a17276d45a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.phobert.tokenization_phobert.PhobertTokenizer"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b6ddf7-17d5-490e-abff-b6569fa28232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encoder['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98356f92-685c-46f0-98f7-b175af3f9826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6926ccaf-5000-4290-84ee-b7ad5aa244b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedEmbedding(nn.Embedding):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(PretrainedEmbedding, self).__init__(\n",
    "            *args, norm_type=2, **kwargs)\n",
    "        self.vocab = {}\n",
    "        self.i2w = {}\n",
    "        \n",
    "    def from_pretrained(self, \n",
    "                        tokenizer: transformers.PreTrainedTokenizer, \n",
    "                        embedding_matrix: nn.Embedding,\n",
    "                        freeze: bool = True):\n",
    "        self.weight = embedding_matrix.weight\n",
    "        \n",
    "        for i in range(0, len(tokenizer)):\n",
    "            self.i2w[i] = tokenizer.convert_ids_to_tokens(i)\n",
    "            self.vocab[tokenizer.convert_ids_to_tokens(i)] = i\n",
    "        \n",
    "        if freeze:\n",
    "            self.weight.requires_grad = False\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24e99e78-d4f5-44ab-89e3-bc940c20ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = len(tokenizer)\n",
    "embedding_dim = phobert.embeddings.word_embeddings.weight.shape[1]\n",
    "pretrained_emb = PretrainedEmbedding(num_embeddings, embedding_dim).from_pretrained(tokenizer,\n",
    "                                                       phobert.embeddings.word_embeddings,\n",
    "                                                       True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cedb7e31-3c37-43e0-8e5a-639a40223d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingCompressor(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, \n",
    "                         num_codebooks: int, \n",
    "                         num_vectors: int, \n",
    "                         use_gpu: bool = False):\n",
    "        super(EmbeddingCompressor, self).__init__()\n",
    "        self.tau = 1\n",
    "        self.M = num_codebooks\n",
    "        self.K = num_vectors\n",
    "        \n",
    "        self.use_gpu = use_gpu\n",
    "        # E(w) -> h_w\n",
    "        # From the paper: \"In our experiments, the hidden layer h w always has a size of MK/2\n",
    "        self.hidden_layer1 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, \n",
    "                      num_codebooks*num_vectors//2, \n",
    "                      bias = True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.hidden_layer2 = nn.Linear(\n",
    "            num_codebooks*num_vectors//2, num_codebooks*num_vectors, bias = True)\n",
    "        self.codebook = nn.Parameter(torch.FloatTensor(\n",
    "            self.M*self.K, embedding_dim), requires_grad = True)\n",
    "        \n",
    "    def _encode(self, embeddings):\n",
    "        # E(w) -> h_w [B, H] -> [B, M*K/2]\n",
    "        h = self.hidden_layer1(embeddings)\n",
    "        # h_w -> a_w [B, M*K]\n",
    "        logits = F.softplus(self.hidden_layer2(h))\n",
    "        # [B, M * K] -> [B, M, K]\n",
    "        logits = logits.view(-1, self.M, self.K).contiguous()\n",
    "        return logits\n",
    "    \n",
    "    def _decode(self, gumbel_output):\n",
    "        return gumbel_output.matmul(self.codebook)\n",
    "    \n",
    "    def forward(self, vector):\n",
    "        # Encoding [B, M, K]\n",
    "        logits = self._encode(vector)\n",
    "        \n",
    "        # Discretization\n",
    "        D = F.gumbel_softmax(\n",
    "            logits.view(-1, self.K).contiguous(), tau = self.tau, hard=False)\n",
    "        gumbel_output = D.view(-1, self.M*self.K).contiguous()\n",
    "        maxp, _ = D.view(-1, self.M, self.K).max(dim=2)\n",
    "        \n",
    "        # Decode\n",
    "        pred = self._decode(gumbel_output)\n",
    "        return logits, maxp.data.clone().mean(), pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63168491-9cb4-4f10-b958-e65efe50fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, num_embedding, \n",
    "                 embedding_dim, model_path, lr=1e-4, \n",
    "                 use_gpu=False, batch_size=64):\n",
    "        self.model = model\n",
    "        self.embedding = PretrainedEmbedding(num_embeddings, embedding_dim)\n",
    "        self.vocab_size = 0\n",
    "        self.use_gpu = use_gpu\n",
    "        self._batch_size = batch_size\n",
    "        self.optimizer = Adam(model.parameters(), lr=lr)\n",
    "        self._model_path = model_path\n",
    "    def load_pretrained_embedding(self, \n",
    "                        tokenizer: transformers.PreTrainedTokenizer, \n",
    "                        embedding_matrix: nn.Embedding,\n",
    "                        freeze: bool = True):\n",
    "        self.embedding = self.embedding.from_pretrained(tokenizer,\n",
    "                                                       embedding_matrix,\n",
    "                                                       freeze)\n",
    "        self.vocab_size = len(self.embedding.vocab)\n",
    "    def run(self, max_epochs=200):\n",
    "        torch.manual_seed(3)\n",
    "        criteration = nn.MSELoss(reduction=\"sum\")\n",
    "        assert (self.vocab_size > 0)\n",
    "        valid_ids = torch.from_numpy(np.random.randint(\n",
    "            0, self.vocab_size, (self._batch_size*10, ))).long()\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        vocab_list = [x for x in range(self.vocab_size)]\n",
    "        for epoch in range(max_epochs):\n",
    "            self.model.train()\n",
    "            time_start = time.time()\n",
    "            random.shuffle(vocab_list)\n",
    "            train_loss_list = []\n",
    "            train_maxp_list = []\n",
    "            \n",
    "            # for start_idx in range(0, self.vocab_size, self._batch_size):\n",
    "            for start_idx in range(0, 2, self._batch_size):\n",
    "                word_ids = torch.Tensor(vocab_list[start_idx:start_idx + self._batch_size]).long()\n",
    "                self.optimizer.zero_grad()\n",
    "                input_embeds = self.embedding(word_ids)\n",
    "                if self.use_gpu:\n",
    "                    input_embeds = input_embeds.cuda()\n",
    "                logits, maxp, pred = self.model(input_embeds)\n",
    "                loss = criteration(pred, input_embeds).div(self._batch_size)\n",
    "                train_loss_list.append(loss.data.clone().item())\n",
    "                train_maxp_list.append(maxp.cpu() if self.use_gpu else maxp)\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(self.model.parameters(), 0.001)\n",
    "                self.optimizer.step()\n",
    "            time_elapsed = time.time() - time_start\n",
    "            train_loss = np.mean(train_loss_list)/2 # why divide by 2? mse loss\n",
    "            train_maxp = np.mean(train_maxp_list)\n",
    "            \n",
    "            self.model.eval()\n",
    "            val_loss_list = []\n",
    "            val_maxp_list = []\n",
    "            \n",
    "            # for start_idx in range(0, len(valids_id), self._batch_size):\n",
    "            for start_idx in range(0, 2, self._batch_size):\n",
    "                word_ids = valid_ids[start_idx:start_idx + self._batch_size]\n",
    "                \n",
    "                oracle = self.embedding(word_ids)\n",
    "                if self.use_gpu:\n",
    "                    oracle = oracle.cuda()\n",
    "                logits, maxp, pred = self.model(oracle)\n",
    "                loss = criteration(pred, oracle).div(self._batch_size)\n",
    "                val_loss_list.append(loss.data.clone().item())\n",
    "                val_maxp_list.append(maxp.cpu() if self.use_gpu else maxp)\n",
    "                \n",
    "            val_loss = np.mean(val_loss_list)/2\n",
    "            val_maxp = np.mean(val_maxp_list)\n",
    "            \n",
    "            if train_loss < best_loss*0.99:\n",
    "                best_loss = train_loss\n",
    "                print(\"[epoch {}] trian_loss={:.2f}, train_maxp={:.2f}, valid_loss={:.2f}, valid_maxp={:.2f},  bps={:.0f} \".format(\n",
    "                    epoch, train_loss, train_maxp,\n",
    "                    val_loss, val_maxp,\n",
    "                    len(train_loss_list) / time_elapsed\n",
    "                ))\n",
    "        print('Training done!')\n",
    "    def export(self, prefix):\n",
    "        assert os.path.exists(self._model_path + \".pt\")\n",
    "        vocab_list = list(range(self.vocab_size))\n",
    "        \n",
    "        codebook = dict(self.model.named_parameters())['codebook'].data\n",
    "        if self.use_gpu:\n",
    "            codebook = codebook.cpu()\n",
    "        \n",
    "        np.save(prefix + \".codebook\", codebook.numpy())\n",
    "    \n",
    "        with open(prefix + \".codes\", \"w+\", encoding='utf-8') as fout:\n",
    "            vocab_list = list(range(self.vocab_size))\n",
    "            # for start_idx in tqdm(range(0, vocab_size, self._batch_size)):\n",
    "            for start_idx in tqdm(range(0, 2, self._batch_size)):\n",
    "                word_ids = torch.Tensor(\n",
    "                    vocab_list[start_idx:start_idx + self._batch_size]).long() \n",
    "                input_embeds = self.embedding(word_ids)\n",
    "                if self.use_gpu:\n",
    "                    input_embeds = input_embeds.cuda()\n",
    "                logits = self.model._encode(input_embeds)\n",
    "                _, codes = logits.max(dim=2)\n",
    "\n",
    "                for w_id, code in zip(word_ids, codes):\n",
    "                    w_id = w_id.item()\n",
    "                    if self.use_gpu:\n",
    "                        code = code.data.cpu().tolist()\n",
    "                    else:\n",
    "                        code = code.data.tolist()\n",
    "                    word = self.embedding.i2w[w_id]\n",
    "                    fout.write(word + \"\\t\" + \" \".join(map(str, code)) + \"\\n\")\n",
    "    def evaluate(self):\n",
    "        assert os.path.exists(self._model_path + \".pt\")\n",
    "        vocab_list = list(range(self.vocab_size))\n",
    "        distances = []\n",
    "        # for start_idx in range(0, len(valids_id), self._batch_size):\n",
    "        for start_idx in range(0, 2, self._batch_size):\n",
    "            word_ids = torch.Tensor(\n",
    "                    vocab_list[start_idx:start_idx + self._batch_size]).long()\n",
    "\n",
    "            input_embeds = self.embedding(word_ids)\n",
    "            if self.use_gpu:\n",
    "                input_embeds = input_embeds.cuda()\n",
    "            _, _, recontructed = self.model(input_embeds)\n",
    "            \n",
    "            distances.extend(np.linalg.norm((recontructed - input_embeds).data.cpu(), axis=1).tolist())\n",
    "\n",
    "        return np.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "087e6b6a-6b87-46a7-b30e-6b7bf250c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_embeddings = len(tokenizer)\n",
    "# embedding_dim = phobert.embeddings.word_embeddings.weight.shape[1] 768\n",
    "compressor = EmbeddingCompressor(embedding_dim=768, num_codebooks=32, num_vectors=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "218f76fd-15b6-4ddd-8188-d5ed37467688",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = len(tokenizer)\n",
    "trainer = Trainer(compressor, \n",
    "                  num_embedding=num_embeddings, \n",
    "                  embedding_dim=768, model_path=\"test\", lr=1e-4, \n",
    "                  use_gpu=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc40fa80-372c-4768-b8b1-63170e07fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_pretrained_embedding(tokenizer, phobert.embeddings.word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d2279f0-c991-4f89-aa2f-b44045ae678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] trian_loss=5.65, train_maxp=0.43, valid_loss=5.69, valid_maxp=0.43,  bps=16 \n",
      "[epoch 2] trian_loss=5.59, train_maxp=0.42, valid_loss=5.61, valid_maxp=0.43,  bps=22 \n",
      "[epoch 8] trian_loss=5.51, train_maxp=0.43, valid_loss=5.41, valid_maxp=0.42,  bps=26 \n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "trainer.run(max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c87108bf-4169-4227-b6b9-5be1a53f45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainer.model.state_dict(), \"test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d3ddc22-50e4-4879-b0f2-6633c01b9564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 359.56it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.export(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46ba49bd-4ef0-4e6f-8c8a-1040496b57e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4702485762536526"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
